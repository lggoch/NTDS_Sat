{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:02.326137Z",
     "start_time": "2018-01-21T23:44:01.520173Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.sparse.linalg\n",
    "from scipy import sparse, stats, spatial\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    "\n",
    "from helpers_satcat import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "START HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# SATCAT launch site analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this analysis, we will attempt to identify from which launch site a satellite has been launched from informations provided by the SATCAT database, such as its orbital parameters, country of origin, nature, etc.\n",
    "\n",
    "The goal will be to identify satellites launch site by using only a subset of labeled satellites.\n",
    "We want to see what type of precision we can obtain with graph-based analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SATCAT data extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have a cleaner notebook, we extracted the data from the SATCAT dataset using an external script, called *create_satcat_json.py*.\n",
    "\n",
    "This script generates a json file containing the formatted SATCAT data and more complete information on the files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract celestrack SATCAT data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:02.589218Z",
     "start_time": "2018-01-21T23:44:02.327737Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"Dataset/satcat_info.json\") as f:\n",
    "    satcat_info_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:02.719030Z",
     "start_time": "2018-01-21T23:44:02.590679Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the main dataset and put it in a dataframe\n",
    "sat_info_array = satcat_info_json[\"sat_data\"]\n",
    "satcat_df = pd.DataFrame(sat_info_array)\n",
    "satcat_df = satcat_df.set_index(\"NORAD\")\n",
    "display(satcat_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:02.724286Z",
     "start_time": "2018-01-21T23:44:02.721149Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract complementary information dictionary\n",
    "operational_status_dict = satcat_info_json[\"operational_status\"]\n",
    "launch_site_full_name_dict = satcat_info_json[\"launch_site\"]\n",
    "source_full_name_dict = satcat_info_json[\"source\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T15:17:32.130593Z",
     "start_time": "2018-01-20T15:17:32.127895Z"
    }
   },
   "source": [
    "### Fill NaN in satcat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:02.794862Z",
     "start_time": "2018-01-21T23:44:02.726351Z"
    }
   },
   "outputs": [],
   "source": [
    "satcat_df = satcat_df.fillna(value=0)\n",
    "display(satcat_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we want to see how the different parameters of the satellites behave.\n",
    "\n",
    "To do so, we will analyze the various parameters.\n",
    "We used a systematic approach, where for each parameter of interest, we did a short analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch site "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:02.805669Z",
     "start_time": "2018-01-21T23:44:02.796923Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dict to transform launch site to numerical value\n",
    "num_launch_site_dict = {}\n",
    "for index,site in enumerate(satcat_df.launch_site.unique()):\n",
    "    num_launch_site_dict[site] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:02.842510Z",
     "start_time": "2018-01-21T23:44:02.807762Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_launch_site = satcat_df.launch_site.map(lambda x:num_launch_site_dict[x])\n",
    "num_launch_site.name = \"num_launch_site\"\n",
    "satcat_df = pd.concat([satcat_df,num_launch_site], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:02.859483Z",
     "start_time": "2018-01-21T23:44:02.844327Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# How many launch per site\n",
    "launches_per_site = satcat_df.launch_site.value_counts()\n",
    "display(launches_per_site.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:03.179062Z",
     "start_time": "2018-01-21T23:44:02.860971Z"
    },
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(launches_per_site.values,kde=False, rug=True)\n",
    "plt.savefig(\"fig/satcat_launches_per_site.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T12:52:57.679109Z",
     "start_time": "2018-01-20T12:52:57.676989Z"
    }
   },
   "source": [
    "### Country of origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:03.186721Z",
     "start_time": "2018-01-21T23:44:03.180710Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dict to transform launch site to numerical value\n",
    "num_source_dict = {}\n",
    "for index,site in enumerate(satcat_df.source.unique()):\n",
    "    num_source_dict[site] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:03.207866Z",
     "start_time": "2018-01-21T23:44:03.188057Z"
    }
   },
   "outputs": [],
   "source": [
    "# How many launch per sources\n",
    "launches_per_source = satcat_df.source.value_counts()\n",
    "display(launches_per_source.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:03.629811Z",
     "start_time": "2018-01-21T23:44:03.209301Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(launches_per_source.values,kde=False, rug=True)\n",
    "plt.savefig(\"fig/satcat_launches_per_source_count.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operational Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:03.638863Z",
     "start_time": "2018-01-21T23:44:03.631779Z"
    }
   },
   "outputs": [],
   "source": [
    "num_operational_status_dict = {}\n",
    "for index, status in enumerate(satcat_df.operational_status.unique()):\n",
    "    num_operational_status_dict[status] = index\n",
    "print(num_operational_status_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:03.658358Z",
     "start_time": "2018-01-21T23:44:03.640426Z"
    }
   },
   "outputs": [],
   "source": [
    "# How many satellites per operational status\n",
    "operational_status_count = satcat_df.operational_status.value_counts()\n",
    "display(operational_status_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:03.814387Z",
     "start_time": "2018-01-21T23:44:03.659691Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(operational_status_count.values,kde=False, rug=True)\n",
    "plt.savefig(\"fig/satcat_operational_status_count.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orbital status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See: https://celestrak.com/satcat/satcat-format.asp \n",
    "Orbital Status Code for the meaning of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:03.822187Z",
     "start_time": "2018-01-21T23:44:03.816100Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "orbital_statuses = satcat_df.orbital_status.unique()\n",
    "print(len(orbital_statuses), \"\\n\", orbital_statuses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:03.830161Z",
     "start_time": "2018-01-21T23:44:03.823483Z"
    }
   },
   "outputs": [],
   "source": [
    "num_orbital_status_dict = {}\n",
    "for index, status in enumerate(orbital_statuses):\n",
    "    num_orbital_status_dict[status] = index\n",
    "print(num_orbital_status_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:03.848548Z",
     "start_time": "2018-01-21T23:44:03.831630Z"
    }
   },
   "outputs": [],
   "source": [
    "# How many satellites per orbital status\n",
    "orbital_status_count = satcat_df.orbital_status.value_counts()\n",
    "display(orbital_status_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:04.100568Z",
     "start_time": "2018-01-21T23:44:03.850480Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(orbital_status_count.values,kde=False, rug=True)\n",
    "plt.savefig(\"fig/satcat_orbital_status_count.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orbital Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T21:58:40.511333Z",
     "start_time": "2018-01-21T21:58:40.508795Z"
    }
   },
   "source": [
    "For each orbital parameters we want to do a distribution analysis by launch site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:04.104328Z",
     "start_time": "2018-01-21T23:44:04.101920Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ORBITAL_PARAMETERS_COLS = [\"apogee\", \"inclination\", \"launch_year\", \"orbital_period\", \"perigee\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:07.921776Z",
     "start_time": "2018-01-21T23:44:04.105662Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(ORBITAL_PARAMETERS_COLS), 1, figsize=(10,20))\n",
    "\n",
    "for index, parameter in enumerate(ORBITAL_PARAMETERS_COLS):\n",
    "    axes[index].hist(satcat_df[parameter].values, bins=100);\n",
    "    axes[index].set_xscale(\"log\")\n",
    "    axes[index].set_yscale(\"log\")\n",
    "    axes[index].set_title(parameter)\n",
    "plt.savefig(\"fig/satcat_orbital_params.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data reduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do clustering, we want to reduce the complexity of the target clusters. \n",
    "\n",
    "To do so, we will first reduce the number of launch sites and/or sources.\n",
    "\n",
    "We tried two methods:\n",
    "\n",
    "    1. Using percentiles of the value counts, so it can be easily adapted depending on the desired results.\n",
    "    2. Using specific launch sites\n",
    "\n",
    "The method using specific launch sites \"AFETR\" and \"AFWTR\", which where the main launch sites for the United-States of America, gave the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:07.926710Z",
     "start_time": "2018-01-21T23:44:07.923305Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "REDUCE_PER_PERCENTILE = False # Reduction per percentile didn't work as expected\n",
    "REDUCE_PER_LAUNCH_SITE = True # Attempt to segment by country: there might be reasons for using particular sites\n",
    "TARGET_PERCENTILE_LAUNCH_SITES = 90\n",
    "TARGET_PERCENTILE_SOURCES = 90\n",
    "LAUNCH_SITES = [\"AFETR\", \"AFWTR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:07.936399Z",
     "start_time": "2018-01-21T23:44:07.928302Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if REDUCE_PER_PERCENTILE:\n",
    "    reduced_satcat_df = data_reduction_per_percentile(satcat_df, TARGET_PERCENTILE_LAUNCH_SITES, TARGET_PERCENTILE_SOURCES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:07.991196Z",
     "start_time": "2018-01-21T23:44:07.938194Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if REDUCE_PER_LAUNCH_SITE:\n",
    "    reduced_satcat_df = data_reduction_per_launch_site(satcat_df, LAUNCH_SITES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:08.014358Z",
     "start_time": "2018-01-21T23:44:07.992785Z"
    }
   },
   "outputs": [],
   "source": [
    "print(reduced_satcat_df.source.unique())\n",
    "display(reduced_satcat_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce by payload or operational"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce even more the size of the dataset, we can chose to keep only payloads and/or operational satellites in the dataset.\n",
    "\n",
    "We chose to keep only payloads, because the dataset has many other types, such as debris and launchers, which are not really interesting for our analysis.\n",
    "\n",
    "We chose to use all historic satellites to have more data on which to work.\n",
    "Also historic satellites are likely to follow similar orbits as modern satellites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:08.026060Z",
     "start_time": "2018-01-21T23:44:08.016335Z"
    }
   },
   "outputs": [],
   "source": [
    "ONLY_PAYLOAD=True\n",
    "ONLY_OPERATIONAL=False\n",
    "print(\"Length before further reduction: \", len(reduced_satcat_df))\n",
    "if ONLY_PAYLOAD:\n",
    "    reduced_satcat_df = reduced_satcat_df.loc[reduced_satcat_df.payload_flag == True]\n",
    "if ONLY_OPERATIONAL:\n",
    "    reduced_satcat_df = reduced_satcat_df.loc[reduced_satcat_df.operational_status == \"Operational\"]\n",
    "    \n",
    "print(\"Length after reduction: \", len(reduced_satcat_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features and Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Features DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to extract the features for each satellite. \n",
    "As a first glance, we want to keep as much data as possible. Later on we might drop some features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:08.052552Z",
     "start_time": "2018-01-21T23:44:08.027427Z"
    }
   },
   "outputs": [],
   "source": [
    "display(reduced_satcat_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:08.064404Z",
     "start_time": "2018-01-21T23:44:08.053887Z"
    }
   },
   "outputs": [],
   "source": [
    "satcat_df.orbital_status.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:08.072123Z",
     "start_time": "2018-01-21T23:44:08.065925Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reduced_satcat_df.orbital_status.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:08.111244Z",
     "start_time": "2018-01-21T23:44:08.073697Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note: because this function is key to understanding how we create the features, we kept it in the notebook\n",
    "\n",
    "def get_feature_dataframe(reduced_satcat_df, only_payload=True, only_operational=False):\n",
    "    \"\"\"Function to create the feature dataframe\"\"\"\n",
    "    # We keep all the features that could have an impact on the clustering\n",
    "    # Note: We do this in a general fashion, so we keep features that could have been reduced just in case\n",
    "\n",
    "    # Numeric Features : \"apogee\", \"inclination\", \"launch_year\", \"orbital_period\", \"perigee\", \"radar_cross_section\"\n",
    "    # Boolean Features : \"payload_flag\"\n",
    "    # List Features    : \"operational_status\", \"orbital_status\", \"source\"\n",
    "    numeric_features = [\"apogee\", \"inclination\", \"launch_year\", \"orbital_period\", \"perigee\", \"radar_cross_section\"]\n",
    "    boolean_features = [\"payload_flag\"]\n",
    "    list_features = [\"operational_status\", \"orbital_status\", \"source\"]\n",
    "    features_columns = numeric_features + boolean_features + list_features\n",
    "\n",
    "    # Numeric features don't require special management\n",
    "    features_df = reduced_satcat_df[numeric_features]\n",
    "    display(features_df.head(5))\n",
    "\n",
    "    # Transform boolean features to numeric\n",
    "    num_payload_flag = reduced_satcat_df.payload_flag.map (\\\n",
    "                                lambda x : 1 if x else 0\n",
    "                       )\n",
    "    if not only_payload:\n",
    "        features_df = features_df.assign(payload_flag = num_payload_flag)\n",
    "\n",
    "    # We need to transform the List features in a numerical form, we will use the unique value index to do so\n",
    "    # We previously created indexes to be able to find them easily\n",
    "    # \"operational_status\" : num_operational_status_dict\n",
    "    # \"orbital_status\" : num_orbital_status_dict\n",
    "    # \"source\": num_source_dict\n",
    "    num_operational_status = reduced_satcat_df.operational_status.map( \\\n",
    "                                    lambda x : num_operational_status_dict[x] \\\n",
    "                             )\n",
    "    num_orbital_status = reduced_satcat_df.orbital_status.map( \\\n",
    "                                    lambda x : num_orbital_status_dict[x] \\\n",
    "                             )\n",
    "    num_source = reduced_satcat_df.source.map( \\\n",
    "                                    lambda x : num_source_dict[x] \\\n",
    "                             )\n",
    "    if not only_operational:\n",
    "        features_df = features_df.assign(operational_status = num_operational_status)\n",
    "    features_df = features_df.assign(orbital_status = num_orbital_status)\n",
    "    features_df = features_df.assign(source = num_source)\n",
    "    \n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:08.152074Z",
     "start_time": "2018-01-21T23:44:08.112778Z"
    }
   },
   "outputs": [],
   "source": [
    "features_df = get_feature_dataframe(reduced_satcat_df, ONLY_PAYLOAD, ONLY_OPERATIONAL)\n",
    "display(features_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T15:12:58.239430Z",
     "start_time": "2018-01-20T15:12:58.235622Z"
    }
   },
   "source": [
    "### Normalize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:08.173263Z",
     "start_time": "2018-01-21T23:44:08.153823Z"
    }
   },
   "outputs": [],
   "source": [
    "features_df = normalize_features(features_df)\n",
    "display(features_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Label Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:08.182477Z",
     "start_time": "2018-01-21T23:44:08.175226Z"
    }
   },
   "outputs": [],
   "source": [
    "num_launch_site = reduced_satcat_df.num_launch_site\n",
    "label_df = pd.DataFrame(num_launch_site)\n",
    "display(label_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph data visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the features we calculate, we want to find the distance between the feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:08.214242Z",
     "start_time": "2018-01-21T23:44:08.183967Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distance_metric = \"braycurtis\"\n",
    "distances = spatial.distance.squareform(spatial.distance.pdist(features_df, metric=distance_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:08.594442Z",
     "start_time": "2018-01-21T23:44:08.215692Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(distances.reshape(-1), bins=100)\n",
    "plt.savefig(\"fig/satcat_distances_hist.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:08.604066Z",
     "start_time": "2018-01-21T23:44:08.596139Z"
    }
   },
   "outputs": [],
   "source": [
    "print('{} distances equal exactly zero.'.format(np.sum(distances == 0)))\n",
    "print(len(distances))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:08.691044Z",
     "start_time": "2018-01-21T23:44:08.605343Z"
    }
   },
   "outputs": [],
   "source": [
    "KERNEL_WIDTH_PERCENTILE = 0.5\n",
    "weights = get_weights_from_distance(distances, KERNEL_WIDTH_PERCENTILE)\n",
    "# Validate that all the weights on the diagonal are null\n",
    "print(np.sum(np.diagonal(weights)==0))\n",
    "print(len(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:09.388164Z",
     "start_time": "2018-01-21T23:44:08.692631Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the resulting weight value\n",
    "fix, axes = plt.subplots(1, 2, figsize=(17, 8))\n",
    "plot_weight_hist(weights, axes[:], name=\"satcat_base_weight_hist\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparcify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:16.052063Z",
     "start_time": "2018-01-21T23:44:09.389945Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "NEIGHBORS = 100\n",
    "epsilon = 1e-8\n",
    "\n",
    "weights = sparse_weights(weights, NEIGHBORS, epsilon)\n",
    "fix, axes = plt.subplots(1, 2, figsize=(17, 8))\n",
    "plot_weight_hist(weights, axes[:], name=\"satcat_sparse_weight_hist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T15:43:50.425035Z",
     "start_time": "2018-01-20T15:43:50.422806Z"
    }
   },
   "source": [
    "### Graph laplacian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph laplacian can give use meaningfull information on the nodes distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:16.338991Z",
     "start_time": "2018-01-21T23:44:16.053719Z"
    }
   },
   "outputs": [],
   "source": [
    "degrees = np.sum(weights, axis=1) \n",
    "plt.hist(degrees, bins=50);\n",
    "plt.savefig(\"fig/satcat_laplacian_degree.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:23.923074Z",
     "start_time": "2018-01-21T23:44:16.340552Z"
    }
   },
   "outputs": [],
   "source": [
    "laplacian = np.zeros((len(degrees), len(degrees)))\n",
    "laplacian.flags.writeable = True\n",
    "\n",
    "for i in tqdm(range(len(degrees))):\n",
    "    for j in range(len(degrees)):\n",
    "        #print(i,j, flush=True)\n",
    "        if i==j:\n",
    "            laplacian[i][j] = 1\n",
    "        else:\n",
    "            square_root = np.sqrt(degrees[i]*degrees[j])\n",
    "            # Avoid divide by 0\n",
    "            # Check if we should put 1 or 0 here...\n",
    "            if square_root == 0.0:\n",
    "                laplacian[i][j] = 0.0\n",
    "            else:\n",
    "                laplacian[i][j] = -weights[i][j]/square_root\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:24.623452Z",
     "start_time": "2018-01-21T23:44:23.924478Z"
    }
   },
   "outputs": [],
   "source": [
    "fix, axes = plt.subplots(1, 2, figsize=(10, 10))\n",
    "axes[0].spy(laplacian);\n",
    "axes[0].set_title(\"Laplacian matrix\")\n",
    "axes[1].spy(weights)\n",
    "axes[1].set_title(\"Weights matrix\")\n",
    "plt.savefig(\"fig/laplacian_and_weight_matrix.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:24.662836Z",
     "start_time": "2018-01-21T23:44:24.625429Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "laplacian = sparse.csr_matrix(laplacian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:33.558908Z",
     "start_time": "2018-01-21T23:44:24.664836Z"
    }
   },
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = sparse.linalg.eigsh(laplacian,\n",
    "                                                k=len(weights)-1,\n",
    "                                                maxiter=100000,\n",
    "                                                which='SA')\n",
    "plt.plot(eigenvalues, '.-', markersize=5);\n",
    "plt.savefig(\"fig/satcat_laplacian_eigenvalues.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:33.564857Z",
     "start_time": "2018-01-21T23:44:33.560103Z"
    }
   },
   "outputs": [],
   "source": [
    "min_index = 0\n",
    "for index, value in enumerate(eigenvalues):\n",
    "    if value > 0.01:\n",
    "        min_index = index\n",
    "        break\n",
    "print(min_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:35.865083Z",
     "start_time": "2018-01-21T23:44:33.567563Z"
    }
   },
   "outputs": [],
   "source": [
    "NBR_OF_GRAPH = 10\n",
    "fig, axes = plt.subplots(NBR_OF_GRAPH//2,2,figsize=(10,15))\n",
    "for index in range(NBR_OF_GRAPH):\n",
    "    x = eigenvectors[:, index]\n",
    "    y = eigenvectors[:, index+1]\n",
    "    axes[index//2][index %2].scatter(x, y, c=label_df.num_launch_site.values, cmap='Set1', alpha=0.5);\n",
    "    axes[index//2][index %2].set_title(\"{}\".format(index))\n",
    "plt.savefig(\"fig/satcat_laplacian_results.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from the Laplacian analysis that there's signs of clustering in the data, but we don't obtain a simple clustering with data to the left and to the right.\n",
    "\n",
    "To identify the actual clustering, we will need to do more complete network analysis.\n",
    "We will use networkx as the main tool for this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:36.098391Z",
     "start_time": "2018-01-21T23:44:35.866588Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = create_graph_from_weights(weights, reduced_satcat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:36.104112Z",
     "start_time": "2018-01-21T23:44:36.100332Z"
    }
   },
   "outputs": [],
   "source": [
    "print(graph.node[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:50.644510Z",
     "start_time": "2018-01-21T23:44:36.105714Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,1, figsize=(10,5))\n",
    "draw_graph(graph, axes, reduced_satcat_df, name=\"base_graph\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T20:34:21.609105Z",
     "start_time": "2018-01-20T20:34:21.606927Z"
    }
   },
   "source": [
    "### Filter the graph degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first remove nodes with small amount of edges, because we can see that they don't really provide information on the clusters.\n",
    "\n",
    "**NOTE:** \n",
    "\n",
    "    1.This is used only to remove the nodes with a degree of 0, because we want to keep small degree nodes for the next steps.\n",
    "    2.We keep the code to show how we progressed through the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:44:50.660585Z",
     "start_time": "2018-01-21T23:44:50.646212Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# Filter out lonely nodes\n",
    "MINIMUM_DEGREE = 0 \n",
    "print(len(graph.nodes))\n",
    "graph = remove_lonely_nodes(graph, minimum_degree = MINIMUM_DEGREE)\n",
    "print(len(graph.nodes))\n",
    "# Do it again to remove resulting nodes with degree of 0\n",
    "# Only if the minimum degree is greater than 0, otherwise \n",
    "# it would simply repeat the same thing\n",
    "if MINIMUM_DEGREE > 0:\n",
    "    graph = remove_lonely_nodes(graph, minimum_degree = 0)\n",
    "print(len(graph.nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:45:04.139706Z",
     "start_time": "2018-01-21T23:44:50.662102Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,1, figsize=(10,5))\n",
    "draw_graph(graph, axes, reduced_satcat_df, name=\"cleaned_base_graph\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there's clearly some major clusters appearing in the graph. \n",
    "The next step will be to attempt to segregate those clusters and extract see if we can identify they came from which launch site, and to which precision we can identify them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate the graph into connected subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:45:04.596833Z",
     "start_time": "2018-01-21T23:45:04.141347Z"
    }
   },
   "outputs": [],
   "source": [
    "connected_subgraphs = []\n",
    "for subgraph in  nx.connected_component_subgraphs(graph):\n",
    "    connected_subgraphs.append(nx.Graph(subgraph))\n",
    "print(len(connected_subgraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:45:04.933223Z",
     "start_time": "2018-01-21T23:45:04.598302Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(10,5))\n",
    "print_subgraphs_nodes_dist(connected_subgraphs, axes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We node that there's a lot of small graphs, without many nodes.\n",
    "For the visual analysis, we will only use a subset of the graphs, otherwise there would be too many plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:45:04.943245Z",
     "start_time": "2018-01-21T23:45:04.935305Z"
    }
   },
   "outputs": [],
   "source": [
    "nodes_nbr = get_nodes_nbr(connected_subgraphs)\n",
    "MINIMUM_NBR_OF_NODES = np.percentile(nodes_nbr,\n",
    "                                     80)\n",
    "small_subgraphs = [connected_subgraphs[index] for index, val in enumerate(nodes_nbr) if val < MINIMUM_NBR_OF_NODES]\n",
    "kept_subgraphs = [connected_subgraphs[index] for index, val in enumerate(nodes_nbr) if val >= MINIMUM_NBR_OF_NODES]\n",
    "print(len(kept_subgraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:45:05.092958Z",
     "start_time": "2018-01-21T23:45:04.945441Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(5,3))\n",
    "print_subgraphs_nodes_dist(kept_subgraphs, axes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:45:11.797221Z",
     "start_time": "2018-01-21T23:45:05.094509Z"
    }
   },
   "outputs": [],
   "source": [
    "print_subgraphs_network(kept_subgraphs, reduced_satcat_df, name = \"subgraph_networks_before_clique\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T22:10:17.132911Z",
     "start_time": "2018-01-20T22:10:17.130582Z"
    }
   },
   "source": [
    "#### Separate the biggest graph in subgraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T22:12:57.557845Z",
     "start_time": "2018-01-20T22:12:57.554359Z"
    }
   },
   "source": [
    "We see from the result above that most subgraphs are well separated but for the largest subgraph.\n",
    "\n",
    "To improve the results, we will separate the bigger subgraphs into smaller ones.\n",
    "\n",
    "To determine how we will end the separation, we will separate the bigger graphs in cliques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:45:11.802493Z",
     "start_time": "2018-01-21T23:45:11.798479Z"
    }
   },
   "outputs": [],
   "source": [
    "MAXIMUM_SUBGRAPH_NODES_PERCENT = 0.20 # How many nodes you need to be a big subgraph\n",
    "maximum_subgraph_nodes = int(len(graph.nodes)*MAXIMUM_SUBGRAPH_NODES_PERCENT)\n",
    "\n",
    "# Get the index of the big subgraphs\n",
    "big_subgraphs_index = get_big_subgraphs_index(kept_subgraphs,\n",
    "                                              maximum_subgraph_nodes\n",
    "                                             )\n",
    "print(big_subgraphs_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:45:16.428720Z",
     "start_time": "2018-01-21T23:45:11.803850Z"
    }
   },
   "outputs": [],
   "source": [
    "# Segment the big subgraphs into cliques\n",
    "SIZE_OF_SMALLEST_CLIQUE = 20\n",
    "clique_subgraphs = []\n",
    "for subgraph_index in big_subgraphs_index:\n",
    "    current_subgraph = kept_subgraphs[subgraph_index]\n",
    "    current_subgraphs = get_graph_cliques(current_subgraph,\n",
    "                                          smallest_clique=SIZE_OF_SMALLEST_CLIQUE\n",
    "                                         )\n",
    "    clique_subgraphs += current_subgraphs\n",
    "print(get_nodes_nbr(clique_subgraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:45:16.434602Z",
     "start_time": "2018-01-21T23:45:16.430286Z"
    }
   },
   "outputs": [],
   "source": [
    "# Construct a new subgraph without the subgraphs that where\n",
    "# divided in cliques, but with the cliques\n",
    "kept_subgraphs_no_big = [subgraph for index, subgraph in enumerate(kept_subgraphs)\n",
    "                         if not index in big_subgraphs_index]\n",
    "print(get_nodes_nbr(kept_subgraphs))\n",
    "print(get_nodes_nbr(kept_subgraphs_no_big))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:45:16.445714Z",
     "start_time": "2018-01-21T23:45:16.436001Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merge the subgraphs\n",
    "subgraphs = clique_subgraphs + kept_subgraphs_no_big + small_subgraphs\n",
    "print(get_nodes_nbr(subgraphs))\n",
    "print(sum(get_nodes_nbr(subgraphs)))\n",
    "print(len(subgraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:45:20.669104Z",
     "start_time": "2018-01-21T23:45:16.447246Z"
    }
   },
   "outputs": [],
   "source": [
    "print_subgraphs_network(subgraphs[:16], reduced_satcat_df, name = \"subgraph_networks_after_clique\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label the subgraphs and the nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T12:26:53.869922Z",
     "start_time": "2018-01-21T12:26:53.867785Z"
    }
   },
   "source": [
    "Although we have used coloring in the graphs to show the launch site of each node, the algorithm doesn't have any information on the actual label to give to each node.\n",
    "\n",
    "In this section, we will provide the algorithm with a subset of the nodes that have an actual label.\n",
    "From there, we will try to give a probability of each subgraph to represent satellites that have been launched by a particular launch site.\n",
    "\n",
    "We will then identify the nodes of each subgraph in bulk by using the most probable launch site.\n",
    "If the probability is equal or we don't know it (i.e. we don't have any node with a label in the graph) we will identify the node launch site as \"unknown\".\n",
    "\n",
    "Finally, we will compare the results with the actual "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T14:10:51.832723Z",
     "start_time": "2018-01-21T14:10:51.830166Z"
    }
   },
   "source": [
    "### Create the labeled subset of nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create an array with all nodes, their label and if they were part of the labeled subset.\n",
    "\n",
    "If they are unidentified, their label will be -1.\n",
    "\n",
    "The label will be the the numerical value given by\n",
    "num_launch_site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:45:20.892412Z",
     "start_time": "2018-01-21T23:45:20.670339Z"
    }
   },
   "outputs": [],
   "source": [
    "PERCENT_OF_LABELED = 30\n",
    "label_df = create_labeled_df(reduced_satcat_df, PERCENT_OF_LABELED) \n",
    "print(label_df.head(5))\n",
    "print(\"Number of labeled sats: {}\".format(label_df.is_labeled.sum()))\n",
    "print(\"Total number of sats: {}\".format(len(label_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T14:54:04.261151Z",
     "start_time": "2018-01-21T14:54:04.258620Z"
    },
    "collapsed": true
   },
   "source": [
    "### Calculate launch site probability of each subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:45:20.983457Z",
     "start_time": "2018-01-21T23:45:20.894240Z"
    }
   },
   "outputs": [],
   "source": [
    "subgraph_probabilities = [get_label_probs(label_df, subgraph) for subgraph in subgraphs]\n",
    "print(subgraph_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the nodes of each subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:45:21.407443Z",
     "start_time": "2018-01-21T23:45:20.984928Z"
    }
   },
   "outputs": [],
   "source": [
    "label_df = identify_nodes_from_prob(label_df, subgraphs)\n",
    "display(label_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:45:21.413669Z",
     "start_time": "2018-01-21T23:45:21.408987Z"
    }
   },
   "outputs": [],
   "source": [
    "nodes_nbr = [len(graph.nodes) for graph in subgraphs]\n",
    "sum(nodes_nbr)\n",
    "print(len(nodes_nbr))\n",
    "print(len(label_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:45:25.047624Z",
     "start_time": "2018-01-21T23:45:21.415524Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(label_df.label, kde=False, rug=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the error of the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:46:22.068870Z",
     "start_time": "2018-01-21T23:45:25.049339Z"
    }
   },
   "outputs": [],
   "source": [
    "error = {}\n",
    "values = range(0,101,1)#[0, 1, 2, 5, 10, 20, 40, 60, 80, 90, 100]\n",
    "for percent_labeled in values:\n",
    "    labeled_df = get_labeled_df(reduced_satcat_df, subgraphs, percent_labeled)\n",
    "    error[percent_labeled] = get_error_properties(labeled_df, reduced_satcat_df, show=False)\n",
    "\n",
    "key_of_interest = [0, 1, 2, 5, 10, 20, 40, 60, 80, 100]\n",
    "for key in key_of_interest:\n",
    "    print(\"Fraction of labelized data: {} %, error: {}, unidentified:{}\".format(key,\n",
    "                                                                                error[key][\"good_classification_percent\"],\n",
    "                                                                                error[key][\"total_unknown_label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:46:22.983585Z",
     "start_time": "2018-01-21T23:46:22.136888Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "print_error_graph(error, \"satcat_error_graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we obtain very few errors even with low fraction of labelized node.\n",
    "From around 10% of labels we are above 90% and from around 20% of labels we are above 95%\n",
    "\n",
    "This is a solid result and shows that in this context the clustering methodology we used was efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "END HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do analysis with different satellites\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We programmed the project for any number of launch sites, but tested it with only two.\n",
    "\n",
    "What would be the results with a more complex sites of satellites?\n",
    "\n",
    "Lets try to find out by using the helper function we defined all along the project with different parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use previous parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:48:13.290826Z",
     "start_time": "2018-01-21T23:46:22.985220Z"
    }
   },
   "outputs": [],
   "source": [
    "# REDUCE DATAFRAME PARAMETERS\n",
    "REDUCE_PER_PERCENTILE = False # Reduction per percentile didn't work as expected\n",
    "REDUCE_PER_LAUNCH_SITE = True # Attempt to segment by country: there might be reasons for using particular sites\n",
    "TARGET_PERCENTILE_LAUNCH_SITES = 90\n",
    "TARGET_PERCENTILE_SOURCES = 90\n",
    "LAUNCH_SITES = [\"AFETR\", \"AFWTR\"]\n",
    "ONLY_PAYLOAD=True\n",
    "ONLY_OPERATIONAL=False\n",
    "SIZE_OF_SMALLEST_CLIQUE = 20\n",
    "\n",
    "result_dict_normal_param = calculate_all_values(satcat_df,\n",
    "                         get_feature_dataframe,\n",
    "                         REDUCE_PER_PERCENTILE,\n",
    "                         REDUCE_PER_LAUNCH_SITE,\n",
    "                         TARGET_PERCENTILE_LAUNCH_SITES,\n",
    "                         TARGET_PERCENTILE_SOURCES,\n",
    "                         LAUNCH_SITES,\n",
    "                         ONLY_PAYLOAD,\n",
    "                         ONLY_OPERATIONAL,\n",
    "                         SIZE_OF_SMALLEST_CLIQUE\n",
    "                        )\n",
    "error_normal_param = calculate_error(result_dict_normal_param[\"reduced_satcat_df\"],\n",
    "                                     result_dict_normal_param[\"subgraphs\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use percentile of launch sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:48:13.291488Z",
     "start_time": "2018-01-21T23:44:02.481Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "REDUCE_PER_PERCENTILE = True\n",
    "REDUCE_PER_LAUNCH_SITE = False\n",
    "TARGET_PERCENTILE_LAUNCH_SITES = 80\n",
    "TARGET_PERCENTILE_SOURCES = 0\n",
    "\n",
    "result_dict_launch_sites_perc = calculate_all_values(satcat_df,\n",
    "                         REDUCE_PER_PERCENTILE,\n",
    "                         REDUCE_PER_LAUNCH_SITE,\n",
    "                         TARGET_PERCENTILE_LAUNCH_SITES,\n",
    "                         TARGET_PERCENTILE_SOURCES,\n",
    "                         LAUNCH_SITES,\n",
    "                         ONLY_PAYLOAD,\n",
    "                         ONLY_OPERATIONAL,\n",
    "                         SIZE_OF_SMALLEST_CLIQUE\n",
    "                        )\n",
    "error_launch_sites_perc = calculate_error(result_dict_launch_sites_perc[\"reduced_satcat_df\"],\n",
    "                                     result_dict_launch_sites_perc[\"subgraphs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use more launch sites "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:48:13.292073Z",
     "start_time": "2018-01-21T23:44:02.493Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use top 10 sites with the most launches\n",
    "launches_per_site.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:48:13.292599Z",
     "start_time": "2018-01-21T23:44:02.499Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "REDUCE_PER_PERCENTILE = False\n",
    "REDUCE_PER_LAUNCH_SITE = True\n",
    "LAUNCH_SITES = [\"AFETR\", \"AFWTR\", \"TYMSC\", \"PLMSC\", \"TAISC\", \"FRGUI\", \"SRILR\", \"XICLF\", \"JSC\", \"KYMSC\"]\n",
    "\n",
    "result_dict_many_launch_sites = calculate_all_values(satcat_df,\n",
    "                         REDUCE_PER_PERCENTILE,\n",
    "                         REDUCE_PER_LAUNCH_SITE,\n",
    "                         TARGET_PERCENTILE_LAUNCH_SITES,\n",
    "                         TARGET_PERCENTILE_SOURCES,\n",
    "                         LAUNCH_SITES,\n",
    "                         ONLY_PAYLOAD,\n",
    "                         ONLY_OPERATIONAL,\n",
    "                         SIZE_OF_SMALLEST_CLIQUE\n",
    "                        )\n",
    "error_many_launch_sites = calculate_error(result_dict_many_launch_sites[\"reduced_satcat_df\"],\n",
    "                                     result_dict_many_launch_sites[\"subgraphs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use all satellites "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T23:48:13.293458Z",
     "start_time": "2018-01-21T23:44:02.505Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# REDUCE DATAFRAME PARAMETERS\n",
    "REDUCE_PER_PERCENTILE = True # Reduction per percentile didn't work as expected\n",
    "REDUCE_PER_LAUNCH_SITE = False # Attempt to segment by country: there might be reasons for using particular sites\n",
    "TARGET_PERCENTILE_LAUNCH_SITES = 0\n",
    "TARGET_PERCENTILE_SOURCES = 0\n",
    "LAUNCH_SITES = [\"AFETR\", \"AFWTR\"]\n",
    "ONLY_PAYLOAD=True\n",
    "ONLY_OPERATIONAL=False\n",
    "SIZE_OF_SMALLEST_CLIQUE = 20\n",
    "\n",
    "result_dict_all_sats = calculate_all_values(satcat_df,\n",
    "                         get_feature_dataframe,\n",
    "                         REDUCE_PER_PERCENTILE,\n",
    "                         REDUCE_PER_LAUNCH_SITE,\n",
    "                         TARGET_PERCENTILE_LAUNCH_SITES,\n",
    "                         TARGET_PERCENTILE_SOURCES,\n",
    "                         LAUNCH_SITES,\n",
    "                         ONLY_PAYLOAD,\n",
    "                         ONLY_OPERATIONAL,\n",
    "                         SIZE_OF_SMALLEST_CLIQUE\n",
    "                        )\n",
    "error_all_sats = calculate_error(result_dict_all_sats[\"reduced_satcat_df\"],\n",
    "                                     result_dict_all_sats[\"subgraphs\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "none",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
